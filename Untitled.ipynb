{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9796e31-7dde-43cc-a53f-2cf0a884cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring training dataset:\n",
      "Dataset directory: Dataset/Train/Train\n",
      "Class distribution:\n",
      "  - Healthy: 458 images\n",
      "  - Powdery: 430 images\n",
      "  - Rust: 434 images\n",
      "Total images: 1322\n",
      "Found 1322 images belonging to 3 classes.\n",
      "Found 190 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n",
      "\n",
      "Training Custom CNN Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarpa\\plant_disease_detection\\venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\sarpa\\plant_disease_detection\\venv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5570 - loss: 1.3314  \n",
      "Epoch 1: val_accuracy improved from -inf to 0.36842, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 4s/step - accuracy: 0.5587 - loss: 1.3247 - val_accuracy: 0.3684 - val_loss: 4.2706 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7168 - loss: 0.7876  \n",
      "Epoch 2: val_accuracy improved from 0.36842 to 0.38421, saving model to best_cnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 4s/step - accuracy: 0.7171 - loss: 0.7866 - val_accuracy: 0.3842 - val_loss: 5.3372 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m 8/42\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 3s/step - accuracy: 0.7956 - loss: 0.5411"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Data exploration function\n",
    "def explore_dataset(base_dir):\n",
    "    \"\"\"Explore the dataset and print statistics\"\"\"\n",
    "    classes = os.listdir(base_dir)\n",
    "    total_images = 0\n",
    "    \n",
    "    print(f\"Dataset directory: {base_dir}\")\n",
    "    print(\"Class distribution:\")\n",
    "    \n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(base_dir, cls)\n",
    "        if os.path.isdir(class_dir):\n",
    "            num_images = len([f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))])\n",
    "            total_images += num_images\n",
    "            print(f\"  - {cls}: {num_images} images\")\n",
    "    \n",
    "    print(f\"Total images: {total_images}\")\n",
    "    return classes\n",
    "\n",
    "# 2. Enhanced data augmentation\n",
    "def create_data_generators(img_size=(224, 224), batch_size=32):\n",
    "    \"\"\"Create enhanced data generators with more augmentation techniques\"\"\"\n",
    "    # Training data with extensive augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        brightness_range=[0.7, 1.3]\n",
    "    )\n",
    "    \n",
    "    # Validation and test data only get rescaled\n",
    "    valid_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        'Dataset/Train/Train',\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    validation_generator = valid_test_datagen.flow_from_directory(\n",
    "        'Dataset/Validation/Validation',\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    test_generator = valid_test_datagen.flow_from_directory(\n",
    "        'Dataset/Test/Test',\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, validation_generator, test_generator\n",
    "\n",
    "# 3. Create a more sophisticated custom CNN model\n",
    "def create_custom_cnn(input_shape, num_classes):\n",
    "    \"\"\"Create a deeper custom CNN model with regularization\"\"\"\n",
    "    model = Sequential([\n",
    "        # First block\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second block\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third block\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 4. Create a transfer learning model with EfficientNet\n",
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    \"\"\"Create a model using EfficientNetB0 as a base\"\"\"\n",
    "    # Load the pre-trained model\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Freeze the base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom layers\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 5. Define callback functions\n",
    "def get_callbacks(checkpoint_path):\n",
    "    \"\"\"Define callbacks for training\"\"\"\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return [early_stopping, reduce_lr, checkpoint]\n",
    "\n",
    "# 6. Training function\n",
    "def train_model(model, train_generator, validation_generator, epochs=50, checkpoint_path='best_model.h5'):\n",
    "    \"\"\"Train the model with callbacks\"\"\"\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Get callbacks\n",
    "    callbacks = get_callbacks(checkpoint_path)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "# 7. Evaluate the model\n",
    "def evaluate_model(model, test_generator):\n",
    "    \"\"\"Evaluate the model and generate detailed reports\"\"\"\n",
    "    # Get the true labels\n",
    "    y_true = test_generator.classes\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    y_pred_probs = model.predict(test_generator)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Get class labels\n",
    "    class_indices = test_generator.class_indices\n",
    "    class_labels = {v: k for k, v in class_indices.items()}\n",
    "    \n",
    "    # Create classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=list(class_labels.values())))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(class_labels.values()), yticklabels=list(class_labels.values()))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred, y_pred_probs\n",
    "\n",
    "# 8. Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot the training and validation accuracy and loss\"\"\"\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'], loc='lower right')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper right')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 9. Visualize predictions\n",
    "def visualize_predictions(test_dir, model, num_images=5, img_size=(224, 224)):\n",
    "    \"\"\"Visualize model predictions on random test images\"\"\"\n",
    "    classes = os.listdir(test_dir)\n",
    "    class_dirs = [os.path.join(test_dir, cls) for cls in classes if os.path.isdir(os.path.join(test_dir, cls))]\n",
    "    \n",
    "    plt.figure(figsize=(15, num_images * 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Choose a random class and image\n",
    "        random_class_dir = np.random.choice(class_dirs)\n",
    "        class_name = os.path.basename(random_class_dir)\n",
    "        \n",
    "        image_files = [f for f in os.listdir(random_class_dir) if os.path.isfile(os.path.join(random_class_dir, f))]\n",
    "        random_image = np.random.choice(image_files)\n",
    "        image_path = os.path.join(random_class_dir, random_image)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        img = load_img(image_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(img_array)\n",
    "        predicted_class_idx = np.argmax(prediction[0])\n",
    "        class_indices = {i: cls for i, cls in enumerate(classes)}\n",
    "        predicted_class = class_indices[predicted_class_idx]\n",
    "        \n",
    "        # Plot image with predictions\n",
    "        plt.subplot(num_images, 1, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'True: {class_name}, Predicted: {predicted_class} ({prediction[0][predicted_class_idx]:.4f})')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 10. Main execution\n",
    "def main():\n",
    "    # Set parameters\n",
    "    img_size = (224, 224)\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    \n",
    "    # Explore the dataset\n",
    "    print(\"Exploring training dataset:\")\n",
    "    classes = explore_dataset('Dataset/Train/Train')\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    # Create data generators\n",
    "    train_generator, validation_generator, test_generator = create_data_generators(img_size, batch_size)\n",
    "    \n",
    "    # For CNN model\n",
    "    print(\"\\nTraining Custom CNN Model:\")\n",
    "    cnn_model = create_custom_cnn(input_shape=(*img_size, 3), num_classes=num_classes)\n",
    "    cnn_history, cnn_model = train_model(\n",
    "        cnn_model, \n",
    "        train_generator, \n",
    "        validation_generator, \n",
    "        epochs=epochs, \n",
    "        checkpoint_path='best_cnn_model.h5'\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(cnn_history)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating CNN Model:\")\n",
    "    cnn_y_true, cnn_y_pred, _ = evaluate_model(cnn_model, test_generator)\n",
    "    \n",
    "    # Create transfer learning model\n",
    "    print(\"\\nTraining Transfer Learning Model:\")\n",
    "    # Add missing import for GlobalAveragePooling2D\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "    \n",
    "    tl_model = create_transfer_learning_model(input_shape=(*img_size, 3), num_classes=num_classes)\n",
    "    tl_history, tl_model = train_model(\n",
    "        tl_model, \n",
    "        train_generator, \n",
    "        validation_generator, \n",
    "        epochs=epochs, \n",
    "        checkpoint_path='best_tl_model.h5'\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(tl_history)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating Transfer Learning Model:\")\n",
    "    tl_y_true, tl_y_pred, _ = evaluate_model(tl_model, test_generator)\n",
    "    \n",
    "    # Fine-tune the transfer learning model\n",
    "    print(\"\\nFine-tuning Transfer Learning Model:\")\n",
    "    # Unfreeze some layers for fine-tuning\n",
    "    for layer in tl_model.layers[0].layers[-20:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Compile with a lower learning rate\n",
    "    tl_model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-5),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train for a few more epochs\n",
    "    ft_history = tl_model.fit(\n",
    "        train_generator,\n",
    "        epochs=20,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=get_callbacks('best_ft_model.h5')\n",
    "    )\n",
    "    \n",
    "    # Plot fine-tuning history\n",
    "    plot_training_history(ft_history)\n",
    "    \n",
    "    # Evaluate fine-tuned model\n",
    "    print(\"\\nEvaluating Fine-tuned Model:\")\n",
    "    ft_y_true, ft_y_pred, _ = evaluate_model(tl_model, test_generator)\n",
    "    \n",
    "    # Visualize predictions\n",
    "    print(\"\\nVisualizing predictions:\")\n",
    "    visualize_predictions('Dataset/Test/Test', tl_model)\n",
    "    \n",
    "    # Save the best model\n",
    "    tl_model.save('final_model.h5')\n",
    "    print(\"Final model saved as 'final_model.h5'\")\n",
    "    \n",
    "    # Create a function for making predictions\n",
    "    def predict_image(image_path, model):\n",
    "        \"\"\"Predict the class of a single image\"\"\"\n",
    "        img = load_img(image_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "        \n",
    "        prediction = model.predict(img_array)\n",
    "        predicted_class_idx = np.argmax(prediction[0])\n",
    "        \n",
    "        class_indices = {i: cls for i, cls in enumerate(classes)}\n",
    "        predicted_class = class_indices[predicted_class_idx]\n",
    "        confidence = prediction[0][predicted_class_idx]\n",
    "        \n",
    "        return predicted_class, confidence\n",
    "    \n",
    "    # Example prediction\n",
    "    test_image = 'Dataset/Test/Test/Rust/82f49a4a7b9585f1.jpg'\n",
    "    predicted_class, confidence = predict_image(test_image, tl_model)\n",
    "    print(f\"\\nTest prediction for '{test_image}':\")\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
